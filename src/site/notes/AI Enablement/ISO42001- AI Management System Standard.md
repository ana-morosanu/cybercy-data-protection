---
{"dg-publish":true,"permalink":"/ai-enablement/iso-42001-ai-management-system-standard/","title":["ISO42001- AI Management System Standard"]}
---


This international standard provides a framework for organisations to manage artificial intelligence (AI) responsibly. It helps organisations that develop, provide, or use AI systems to ensure these systems are safe, trustworthy, and aligned with legal and ethical expectations.

The standard is designed for all types of organisations, regardless of size or sector. It can be used by companies that build AI systems, use them in their operations, or offer AI-based services.

The core idea is to create an AI management system that fits into the organisation’s existing structure. This system should help manage risks, ensure compliance, and support responsible innovation.

### **Key Concepts**

The standard encourages organisations to understand their internal and external environment. This includes legal requirements, ethical concerns, and the expectations of customers, regulators, and other stakeholders. It also asks organisations to clearly define their role in the AI ecosystem, whether they are developers, users, or partners.

Leadership plays a central role. Top management must show commitment by setting policies, assigning responsibilities, and ensuring the AI management system is integrated into business processes.

Planning is essential. Organisations must identify risks and opportunities related to AI, set clear objectives, and plan how to achieve them. This includes conducting risk assessments and impact assessments to understand how AI systems might affect people and society.

Support involves making sure the right resources, skills, and awareness are in place. Communication and documentation are also important so that everyone involved understands their responsibilities and how the system works.

Operations must be controlled and monitored. This includes managing changes, ensuring AI systems are used as intended, and keeping records of decisions and actions.

Performance must be evaluated regularly. Internal audits and management reviews help ensure the system is working and improving over time.

When problems occur, organisations must take corrective action and learn from mistakes. Continuous improvement is a key part of the standard.

#### **Context of the Organisation**

**Understanding the Organisation and its context.**

Organisations must identify both external and internal factors that could influence their ability to achieve the goals of their AI management system. This includes considering whether climate change is a relevant issue.

They must define the intended purpose of the AI systems they develop, use or provide and determine their roles in relation to those systems. These roles include being a provider, developer, user, partner or regulator of AI.

Understanding these roles helps determine which parts of the standard apply and to what extent. The organisation should also consider legal, regulatory, cultural and market-related factors that may affect their AI activities.

Additional external factors to consider include:
- Legal requirements, including any restrictions on AI use.
- Regulatory guidance or decisions that affect how AI is interpreted or enforced.
- Incentives or consequences tied to the intended use of AI.
- Cultural values, ethics, and societal norms around AI.
- Competitive trends and innovation in AI products and services.

Internally, the organisation should assess:
- Its governance structure, objectives, and internal policies.
- Any contractual obligations that influence AI use.
- The specific purpose of each AI system in development or operation.

The organisation’s role in data processing (e.g. as a controller or processor of personal data) and any legal obligations related to AI must also be considered.

**Understanding the Needs and Expectations of Interested Parties**

Organisations must identify the individuals, groups, or entities that are relevant to their AI management system. These are known as interested parties and may include customers, regulators, partners, employees, or data subjects.

They must determine what these parties expect or require in relation to the organisation’s use of AI, such as transparency, fairness, or compliance with legal obligations. The organisation must then decide which of these expectations will be addressed through the AI management system.

Some of these expectations may also relate to broader concerns, such as climate change or ethical use of AI.

**Determining the Scope of the AI Management System**

The organisation must define the boundaries of its AI management system. This includes identifying which parts of the business, processes, and AI systems are covered.

When setting the scope, the organisation must consider both the internal and external issues it identified earlier, as well as the needs and expectations of interested parties.

The defined scope must be documented and will guide how the organisation applies the standard’s requirements across leadership, planning, operations, and improvement.

**Establishing the AI Management System**

The organisation must create and maintain a structured AI management system that meets the requirements of the standard. This includes defining the processes involved, how they interact, and how they are monitored and improved.

Each process should have clear inputs, outputs, responsibilities, and performance indicators. The organisation must ensure that resources are available, risks are addressed, and that the system is continually evaluated and improved.

Documented information must be maintained to support operations and demonstrate that processes are being followed as intended.

### **Leadership**
Top management must show clear leadership and commitment to the AI management system. This means they need to make sure the AI policy and goals are in line with the organisation’s overall direction. They should also make sure the AI system is properly built into everyday business activities, that enough resources are provided, and that everyone understands why managing AI well is important. It’s their job to make sure the system works as intended, to encourage staff to play their part, to keep improving things, and to support others in leadership roles to do the same in their areas.

A big part of leadership is creating a culture where AI is used responsibly. This includes making sure people are aware of what responsible AI use looks like and that they follow it.

The AI policy itself must suit the organisation’s purpose, help set clear goals, and show a commitment to meeting requirements and improving over time. This policy needs to be written down, linked to other relevant policies, shared within the organisation, and made available to others when appropriate.

Finally, top management must make sure that roles and responsibilities related to the AI system are clearly assigned and communicated. Someone must be given the job of making sure the system meets the standard and of reporting back to top management on how it’s performing.

### **Planning**
Organisations must plan how to manage risks and opportunities linked to their AI systems. This involves identifying what could go wrong or right, and deciding how to deal with those possibilities. The aim is to make sure the AI management system works as intended, avoids unwanted outcomes, and keeps improving.

To do this, organisations need to set clear criteria for what counts as an acceptable risk. They must assess risks by looking at how likely they are and what impact they could have on people, society, and the organisation itself. Based on this, they should decide which risks need action and how to prioritise them.

Once risks are assessed, the organisation must choose how to treat them. This includes selecting controls to manage the risks, checking these against the standard’s reference list, and adding any extra controls if needed. All of this should be documented in a statement of applicability, which explains why certain controls are included or left out. A risk treatment plan must be created and approved by management.

Organisations also need to assess the broader impact of their AI systems on individuals and society. This includes looking at how the system might be misused and what consequences that could have. The results of these assessments should be documented and considered when managing risks.

Finally, organisations must set AI-related goals that match their policy. These goals should be measurable, monitored, communicated, and updated when needed. Plans should be made for how to achieve these goals, including what needs to be done, who is responsible, what resources are needed, and how success will be measured.

Any changes to the AI management system must be planned properly to ensure they are carried out smoothly and effectively.

### **Support**
This section focuses on what an organisation needs to have in place to support its AI management system effectively. 

**Resources**:
The organisation must identify and provide all the necessary resources to set up, run, maintain and improve its AI management system. This includes things like tools, data, infrastructure and people.

**Competence**:
Anyone whose work affects AI performance must be competent. This means they should have the right education, training or experience. If there are gaps, the organisation must take steps to fill them and check that those steps worked. Records of competence must be kept.

**Awareness**:
People working under the organisation's control must be aware of the AI policy, how their work contributes to the system's success and what could happen if they don't follow the rules.

**Communication**:
The organisation must decide what needs to be communicated about the AI management system, when and how to do it and who needs to know, both inside and outside the organisation.

**Documentation information**:
The AI management system must include:
- documents required by the standard
- any other documents the organisation thinks are necessary for the system to work well

When creating or updating documents, the organisation must make sure they are clearly identified, properly formatted and approved. These documents must be controlled so they are available when needed and protected from misuse or loss. This includes managing access, storage, version control, and disposal. External documents that are important for the system must also be identified and controlled.

### **Operations**
This sections outlines how an organisation should manage day-to-day activities of its AI management system.

**Operational Planning and Control**:
The organisation is required to plan, implement, and control the processes necessary to meet the requirements of the AI management system and to carry out the actions identified in Clause 6. This includes establishing criteria for these processes and applying controls accordingly. The controls identified during AI risk treatment (Clause 6.1.3) must be implemented, particularly those related to the AI system’s development and life cycle. The effectiveness of these controls must be monitored, and corrective actions taken if the intended results are not achieved. Documented information must be maintained to confirm that processes are executed as planned. The organisation must also manage both planned and unintended changes, taking action to mitigate any adverse effects. Additionally, any externally provided processes, products, or services relevant to the AI management system must be controlled.

**AI Risk Assessment**:
The organization must conduct AI risk assessments in line with Clause 6.1.2. These assessments should occur at planned intervals or when significant changes are proposed or take place. The results of all AI risk assessments must be documented and retained.

**AI Risk Treatment**:
The AI risk treatment plan, developed under Clause 6.1.3, must be implemented and its effectiveness verified. If new risks are identified or if existing treatments are found to be ineffective, the organisation must revisit the risk treatment process and update the plan accordingly. All outcomes of the risk treatment process must be documented and retained.

**AI System Impact Assessment**:
AI system impact assessments must be performed in accordance with Clause 6.1.4. These assessments should be conducted at planned intervals or when significant changes are proposed. The organisation must document and retain the results of all such assessments.

### **Management Review**
Top management is required to review the organisation's AI management system at planned intervals. The purpose of this review is to ensure that the system remains suitable, adequate and effective in achieving its intended outcomes.

**Management Review Inputs**:
The review must consider several key inputs. These include the status of actions from previous reviews, any changes in internal or external issues relevant to the AI management system, and evolving needs and expectations of interested parties. It must also include performance data from the AI management system, such as trends in nonconformities and corrective actions, monitoring and measurement results and audit findings. Opportunities for continual improvement must also be reviewed.

**Management Review Results**:
The outcomes of the management review must include decisions and actions related to opportunities for continual improvement and any necessary changes to the AI management system. These results must be documented and retained as evidence of the review process.

### **Improvement**

**Continual Improvement**:
The organisation is required to continually improve the suitability, adequacy, and effectiveness of its AI management system. This means that the system should not remain static but should evolve in response to internal evaluations, external changes, and opportunities for enhancement. The goal is to ensure that the AI management system remains aligned with the organisation’s objectives and continues to support responsible and effective use of AI.

**Nonconformity and Corrective Action**:
When a nonconformity occurs, the organisation must take action to address it. This includes reacting to the issue by controlling and correcting it, and dealing with any consequences. The organisation must also evaluate whether further action is needed to eliminate the root cause of the nonconformity to prevent recurrence. This involves reviewing the nonconformity, identifying its causes, and determining whether similar issues exist or could arise elsewhere.

Once actions are identified, they must be implemented and their effectiveness reviewed. If necessary, changes should be made to the AI management system. All actions taken, including the nature of the nonconformities and the results of any corrective actions, must be documented and retained as evidence.

### **Annex A (normative) - Reference Control Objectives and Controls**

##### **A.1 General**
This annex provides a reference set of control objectives and controls to support the implementation and operation of an AI management system. Organisations are not required to implement all listed controls but should select those relevant to their context, risks, and objectives. Additional or alternative controls may also be used.

##### **A.2 Policies Related to AI**
**Objective**: To provide management direction and support for AI systems according to business requirements.
- **A.2.2 AI policy**: The organisation shall document a policy for the development or use of AI systems.
- **A.2.3 Alignment with other organisational policies**: The organisation shall determine where other policies intersect with AI objectives.
- **A.2.4 Review of the AI policy**: The AI policy shall be reviewed periodically to ensure its continued relevance and effectiveness.

##### **A.3 Internal Organisation**
**Objective**: To establish accountability within the organisation for responsible AI system management.
- **A.3.2 AI roles and responsibilities**: Roles and responsibilities for AI shall be clearly defined and allocated.
- **A.3.3 Reporting of concerns**: A process shall be in place for reporting concerns related to AI systems.

##### **A.4 Resources for AI Systems**
**Objective**: To ensure all AI system resources are identified and managed to address risks and impacts.
- **A.4.2 Resource documentation**
- **A.4.3 Data resources**
- **A.4.4 Tooling resources**
- **A.4.5 System and computing resources**
- **A.4.6 Human resources**

##### **A.5 Assessing Impacts of AI Systems**
**Objective**: To assess impacts on individuals, groups, and society throughout the AI system life cycle.
- **A.5.2 AI system impact assessment process**
- **A.5.3 Documentation of AI system impact assessments**
- **A.5.4 Assessing AI system impact on individuals or groups**
- **A.5.5 Assessing societal impacts of AI systems**

##### **A.6 AI System Life Cycle**
**Objective**: To define criteria and requirements for each stage of the AI system life cycle.
- **A.6.1.2 Objectives for responsible development**
- **A.6.1.3 Processes for responsible design and development**
- **A.6.2.2 AI system requirements and specification**
- **A.6.2.3 Documentation of design and development**
- **A.6.2.4 Verification and validation**
- **A.6.2.5 Deployment**
- **A.6.2.6 Operation and monitoring**
- **A.6.2.7 Technical documentation**
- **A.6.2.8 Recording of event logs**

##### **A.7 Data for AI Systems**
**Objective**: To manage data throughout the AI system life cycle.
- **A.7.2 Data for development and enhancement**
- **A.7.3 Acquisition of data**
- **A.7.4 Quality of data**
- **A.7.5 Data provenance**
- **A.7.6 Data preparation**

##### A.8 Information for Interested Parties**
**Objective**: To ensure stakeholders have the necessary information to assess AI system risks and impacts.
- **A.8.2 System documentation and user information**
- **A.8.3 External reporting**
- **A.8.4 Communication of incidents**
- **A.8.5 Information for interested parties**

##### **A.9 Use of AI Systems**
**Objective**: To ensure responsible use of AI systems in line with organisational policies.
- **A.9.2 Processes for responsible use**
- **A.9.3 Objectives for responsible use**
- **A.9.4 Intended use of the AI system**

##### **A.10 Third-Party and Customer Relationships**
**Objective**: To manage responsibilities and risks when third parties are involved in the AI system life cycle.
- **A.10.2 Allocating responsibilities**
- **A.10.3 Suppliers**
- **A.10.4 Customers**

### **Annex B (Normative) – Implementation Guidance for AI Controls**

##### **B.1 General**
Annex B provides implementation guidance for the controls listed in Annex A. This guidance is not mandatory and does not need to be included in the statement of applicability. Organisations may adapt or extend the guidance to suit their specific needs and risk treatment strategies. The annex serves as a starting point for developing organisation-specific implementations of controls.

**B.2 Policies Related to AI**
**B.2.1 Objective**
To provide management direction and support for AI systems according to business requirements.

**B.2.2 AI Policy**
The AI policy should be informed by business strategy, organisational values, risk appetite, legal requirements, and stakeholder impacts. It should include guiding principles and processes for handling exceptions and should reference other relevant policies.

**B.2.3 Alignment with Other Organisational Policies**
Organisations should identify intersections between AI and other domains such as quality, security, safety, and privacy. Policies should be updated or cross-referenced accordingly.

**B.2.4 Review of the AI Policy**
The AI policy should be reviewed periodically or when necessary. A designated role should be responsible for this review, which should consider changes in the business, legal, or technical environment.

##### **B.3 Internal Organisation**

**B.3.1 Objective**
To establish accountability for the implementation, operation, and management of AI systems.

**B.3.2 AI Roles and Responsibilities**
Roles should be defined and assigned based on organisational needs. Areas requiring clear responsibilities include risk management, impact assessments, data quality, oversight, and compliance.

**B.3.3 Reporting of Concerns**
A process should be in place for reporting concerns, with options for anonymity and protection from retaliation. Reports should be escalated appropriately and handled by qualified personnel.

##### **B.4 Resources for AI Systems**

**B.4.1 Objective**
To ensure that all AI system resources are identified and managed to address risks and impacts.

**B.4.2 Resource Documentation**
Organisations should document all relevant resources, including data, tools, systems, and human expertise, using diagrams or architecture models where helpful.

**B.4.3 Data Resources**
Documentation should include data provenance, update history, categories, labeling processes, intended use, quality, retention policies, and known biases.

**B.4.4 Tooling Resources**
Tooling includes algorithms, models, optimisation methods, and development tools. Documentation should cover their use and purpose.

**B.4.5 System and Computing Resources**
Details should include hardware specifications, deployment environments, and environmental impacts. Different stages of the AI life cycle may require different resources.

**B.4.6 Human Resources**
Organisations should document the expertise required for each stage of the AI life cycle, including oversight, development, and maintenance. Diversity and inclusion may be relevant depending on the system’s context.


### **Annex C - Potential AI-related organisational objectives and risk sources**

##### **C.1 General**

Annex C provides a non-exhaustive list of potential organisational objectives and risk sources relevant to AI systems. These are intended to support organisations in identifying and managing risks associated with AI. The relevance of each item depends on the organisation’s context. ISO/IEC 23894 offers further detail on how these elements relate to risk management. Regular evaluation of AI systems helps ensure alignment with organisational objectives.

##### **C.2 Objectives**

**C.2.1 Accountability**
AI can shift traditional accountability structures. Where individuals were once solely responsible for decisions, AI systems may now influence or make those decisions, requiring new accountability frameworks.

**C.2.2 AI Expertise**
Organizations need access to interdisciplinary specialists with skills in assessing, developing, and deploying AI systems to ensure responsible and effective use.

**C.2.3 Availability and Quality of Training and Test Data**
Machine learning-based AI systems require high-quality training, validation, and test data to function as intended. Poor data quality can compromise system performance.

**C.2.4 Environmental Impact**
AI systems can have both positive and negative environmental effects. These include energy consumption, resource use, and potential contributions to sustainability.

**C.2.5 Fairness**
Automated decision-making by AI systems can result in unfair outcomes for certain individuals or groups if not properly managed.

**C.2.6 Maintainability**
Maintainability refers to the organisation’s ability to modify AI systems to correct defects or adapt to new requirements over time.

**C.2.7 Privacy**
AI systems that process personal or sensitive data, such as health records, can pose privacy risks if data is misused or disclosed inappropriately.

**C.2.8 Robustness**
Robustness is the ability of an AI system to maintain consistent performance when exposed to new or unexpected data.

**C.2.9 Safety**
Safety involves ensuring that AI systems do not endanger human life, health, property, or the environment under defined conditions.

**C.2.10 Security**
AI systems, especially those using machine learning, introduce new security challenges beyond traditional IT concerns, such as adversarial attacks or model manipulation.

**C.2.11 Transparency and Explainability**
Transparency refers to the openness of both the organisation and the AI system. Explainability involves providing understandable reasons for AI outputs to stakeholders.

##### **C.3 Risk Sources**

**C.3.1 Complexity of Environment**
AI systems operating in complex environments (e.g. autonomous vehicles) face performance uncertainty due to the wide range of possible scenarios.

**C.3.2 Lack of Transparency and Explainability**
When AI systems cannot provide clear explanations for their decisions, it can undermine trust and accountability.

**C.3.3 Level of Automation**
Higher levels of automation can increase risks in areas such as safety, fairness, and security, especially when human oversight is reduced.

**C.3.4 Risk Sources Related to Machine Learning**
Machine learning introduces risks related to data quality, data poisoning, and the methods used to collect and process data.

**C.3.5 System Hardware Issues**
Hardware-related risks include defects in components or issues when transferring trained models between different hardware platforms.

**C.3.6 System Life Cycle Issues**
Risks can arise at any stage of the AI system life cycle, including design flaws, poor deployment practices, inadequate maintenance, or improper decommissioning.

**C.3.7 Technology Readiness**
Immature technologies may carry unknown risks, while overconfidence in mature technologies can lead to complacency and overlooked vulnerabilities.


### **Annex D - Use of the AI management system across domains or sectors**

##### **D.1 General**

The AI management system defined in ISO/IEC 42001 is applicable to any organisation that develops, provides, or uses products or services involving AI systems. This includes a wide range of sectors such as:
- Health
- Defence
- Transport
- Finance
- Employment
- Energy

Each sector may have its own obligations, best practices, expectations, and contractual commitments. The AI management system offers technology-specific guidance, complementing existing generic or sector-specific management system standards that typically take a technology-neutral approach.

AI systems often incorporate multiple technologies, not just AI-specific components. Therefore, responsible development and use require a holistic approach that considers the entire system. This includes integrating considerations for safety, security, privacy, and environmental impact across all components, not just the AI parts.

To achieve this, organisations should integrate the AI management system with other relevant management systems to ensure comprehensive governance and risk management.

##### **D.2 Integration of AI Management System with Other Management System Standards**

Organisations may have objectives or obligations that align with other management system standards. Integration with these standards enhances consistency and effectiveness. Examples include:

- **ISO/IEC 27001 (Information Security Management)**  
    AI systems often involve sensitive data and critical operations. Integrating ISO/IEC 27001 helps address security objectives systematically. The shared high-level structure between ISO/IEC 27001 and ISO/IEC 42001 facilitates this integration.
    
- **ISO/IEC 27701 (Privacy Information Management)**  
    When AI systems process personally identifiable information (PII), ISO/IEC 27701 helps organisations meet privacy obligations. Privacy-related controls in ISO/IEC 42001 can be aligned with ISO/IEC 27701 implementation.
    
- **ISO 9001 (Quality Management)**  
    Integration with ISO 9001 supports customer confidence and internal effectiveness. It complements AI-specific requirements with broader quality assurance practices, including risk management and supply chain coherence.
    
- **Sector-Specific Standards**  
    Integration is also beneficial with standards tailored to specific industries:
    
    - **ISO 22000** for food safety management (relevant for AI in food production and logistics)
    - **ISO 13485** and **IEC 62304** for medical devices and software (relevant for AI in healthcare)

By aligning the AI management system with these standards, organisations can ensure responsible AI development and use while meeting broader regulatory and operational requirements.
