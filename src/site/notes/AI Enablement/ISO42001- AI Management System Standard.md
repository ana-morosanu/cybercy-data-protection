---
{"dg-publish":true,"permalink":"/ai-enablement/iso-42001-ai-management-system-standard/","title":["ISO42001- AI Management System Standard"]}
---


This international standard provides a framework for organisations to manage artificial intelligence (AI) responsibly. It helps organisations that develop, provide, or use AI systems to ensure these systems are safe, trustworthy, and aligned with legal and ethical expectations.

The standard is designed for all types of organisations, regardless of size or sector. It can be used by companies that build AI systems, use them in their operations, or offer AI-based services.

The core idea is to create an AI management system that fits into the organisation’s existing structure. This system should help manage risks, ensure compliance, and support responsible innovation.

### **Key Concepts**

The standard encourages organisations to understand their internal and external environment. This includes legal requirements, ethical concerns, and the expectations of customers, regulators, and other stakeholders. It also asks organisations to clearly define their role in the AI ecosystem, whether they are developers, users, or partners.

Leadership plays a central role. Top management must show commitment by setting policies, assigning responsibilities, and ensuring the AI management system is integrated into business processes.

Planning is essential. Organisations must identify risks and opportunities related to AI, set clear objectives, and plan how to achieve them. This includes conducting risk assessments and impact assessments to understand how AI systems might affect people and society.

Support involves making sure the right resources, skills, and awareness are in place. Communication and documentation are also important so that everyone involved understands their responsibilities and how the system works.

Operations must be controlled and monitored. This includes managing changes, ensuring AI systems are used as intended, and keeping records of decisions and actions.

Performance must be evaluated regularly. Internal audits and management reviews help ensure the system is working and improving over time.

When problems occur, organisations must take corrective action and learn from mistakes. Continuous improvement is a key part of the standard.

#### **Context of the Organisation**

**Understanding the Organisation and its context.**

Organisations must identify both external and internal factors that could influence their ability to achieve the goals of their AI management system. This includes considering whether climate change is a relevant issue.

They must define the intended purpose of the AI systems they develop, use or provide and determine their roles in relation to those systems. These roles include being a provider, developer, user, partner or regulator of AI.

Understanding these roles helps determine which parts of the standard apply and to what extent. The organisation should also consider legal, regulatory, cultural and market-related factors that may affect their AI activities.

Additional external factors to consider include:
- Legal requirements, including any restrictions on AI use.
- Regulatory guidance or decisions that affect how AI is interpreted or enforced.
- Incentives or consequences tied to the intended use of AI.
- Cultural values, ethics, and societal norms around AI.
- Competitive trends and innovation in AI products and services.

Internally, the organisation should assess:
- Its governance structure, objectives, and internal policies.
- Any contractual obligations that influence AI use.
- The specific purpose of each AI system in development or operation.

The organisation’s role in data processing (e.g. as a controller or processor of personal data) and any legal obligations related to AI must also be considered.

**Understanding the Needs and Expectations of Interested Parties**

Organisations must identify the individuals, groups, or entities that are relevant to their AI management system. These are known as interested parties and may include customers, regulators, partners, employees, or data subjects.

They must determine what these parties expect or require in relation to the organisation’s use of AI, such as transparency, fairness, or compliance with legal obligations. The organisation must then decide which of these expectations will be addressed through the AI management system.

Some of these expectations may also relate to broader concerns, such as climate change or ethical use of AI.

**Determining the Scope of the AI Management System**

The organisation must define the boundaries of its AI management system. This includes identifying which parts of the business, processes, and AI systems are covered.

When setting the scope, the organisation must consider both the internal and external issues it identified earlier, as well as the needs and expectations of interested parties.

The defined scope must be documented and will guide how the organisation applies the standard’s requirements across leadership, planning, operations, and improvement.

**Establishing the AI Management System**

The organisation must create and maintain a structured AI management system that meets the requirements of the standard. This includes defining the processes involved, how they interact, and how they are monitored and improved.

Each process should have clear inputs, outputs, responsibilities, and performance indicators. The organisation must ensure that resources are available, risks are addressed, and that the system is continually evaluated and improved.

Documented information must be maintained to support operations and demonstrate that processes are being followed as intended.

### **Leadership**
Top management must show clear leadership and commitment to the AI management system. This means they need to make sure the AI policy and goals are in line with the organisation’s overall direction. They should also make sure the AI system is properly built into everyday business activities, that enough resources are provided, and that everyone understands why managing AI well is important. It’s their job to make sure the system works as intended, to encourage staff to play their part, to keep improving things, and to support others in leadership roles to do the same in their areas.

A big part of leadership is creating a culture where AI is used responsibly. This includes making sure people are aware of what responsible AI use looks like and that they follow it.

The AI policy itself must suit the organisation’s purpose, help set clear goals, and show a commitment to meeting requirements and improving over time. This policy needs to be written down, linked to other relevant policies, shared within the organisation, and made available to others when appropriate.

Finally, top management must make sure that roles and responsibilities related to the AI system are clearly assigned and communicated. Someone must be given the job of making sure the system meets the standard and of reporting back to top management on how it’s performing.

### **Planning**
Organisations must plan how to manage risks and opportunities linked to their AI systems. This involves identifying what could go wrong or right, and deciding how to deal with those possibilities. The aim is to make sure the AI management system works as intended, avoids unwanted outcomes, and keeps improving.

To do this, organisations need to set clear criteria for what counts as an acceptable risk. They must assess risks by looking at how likely they are and what impact they could have on people, society, and the organisation itself. Based on this, they should decide which risks need action and how to prioritise them.

Once risks are assessed, the organisation must choose how to treat them. This includes selecting controls to manage the risks, checking these against the standard’s reference list, and adding any extra controls if needed. All of this should be documented in a statement of applicability, which explains why certain controls are included or left out. A risk treatment plan must be created and approved by management.

Organisations also need to assess the broader impact of their AI systems on individuals and society. This includes looking at how the system might be misused and what consequences that could have. The results of these assessments should be documented and considered when managing risks.

Finally, organisations must set AI-related goals that match their policy. These goals should be measurable, monitored, communicated, and updated when needed. Plans should be made for how to achieve these goals, including what needs to be done, who is responsible, what resources are needed, and how success will be measured.

Any changes to the AI management system must be planned properly to ensure they are carried out smoothly and effectively.

### **Support**
This section focuses on what an organisation needs to have in place to support its AI management system effectively. 

**Resources**:
The organisation must identify and provide all the necessary resources to set up, run, maintain and improve its AI management system. This includes things like tools, data, infrastructure and people.

**Competence**:
Anyone whose work affects AI performance must be competent. This means they should have the right education, training or experience. If there are gaps, the organisation must take steps to fill them and check that those steps worked. Records of competence must be kept.

**Awareness**:
People working under the organisation's control must be aware of the AI policy, how their work contributes to the system's success and what could happen if they don't follow the rules.

**Communication**:
The organisation must decide what needs to be communicated about the AI management system, when and how to do it and who needs to know, both inside and outside the organisation.

**Documentation information**:
The AI management system must include:
- documents required by the standard
- any other documents the organisation thinks are necessary for the system to work well

When creating or updating documents, the organisation must make sure they are clearly identified, properly formatted and approved. These documents must be controlled so they are available when needed and protected from misuse or loss. This includes managing access, storage, version control, and disposal. External documents that are important for the system must also be identified and controlled.

### **Operations**
This sections outlines how an organisation should manage day-to-day activities of its AI management system.

**Operational Planning and Control**:
The organization is required to plan, implement, and control the processes necessary to meet the requirements of the AI management system and to carry out the actions identified in Clause 6. This includes establishing criteria for these processes and applying controls accordingly. The controls identified during AI risk treatment (Clause 6.1.3) must be implemented, particularly those related to the AI system’s development and life cycle. The effectiveness of these controls must be monitored, and corrective actions taken if the intended results are not achieved. Documented information must be maintained to confirm that processes are executed as planned. The organization must also manage both planned and unintended changes, taking action to mitigate any adverse effects. Additionally, any externally provided processes, products, or services relevant to the AI management system must be controlled.

**AI Risk Assessment**:
The organization must conduct AI risk assessments in line with Clause 6.1.2. These assessments should occur at planned intervals or when significant changes are proposed or take place. The results of all AI risk assessments must be documented and retained.

**AI Risk Treatment**:
The AI risk treatment plan, developed under Clause 6.1.3, must be implemented and its effectiveness verified. If new risks are identified or if existing treatments are found to be ineffective, the organization must revisit the risk treatment process and update the plan accordingly. All outcomes of the risk treatment process must be documented and retained.

**AI System Impact Assessment**:
AI system impact assessments must be performed in accordance with Clause 6.1.4. These assessments should be conducted at planned intervals or when significant changes are proposed. The organization must document and retain the results of all such assessments.

### **Management Review**
Top management is required to review the organisation's AI management system at planned intervals. The purpose of this review is to ensure that the system remains suitable, adequate and effective in achieving its intended outcomes.

**Management Review Inputs**:
The review must consider several key inputs. These include the status of actions from previous reviews, any changes in internal or external issues relevant to the AI management system, and evolving needs and expectations of interested parties. It must also include performance data from the AI management system, such as trends in nonconformities and corrective actions, monitoring and measurement results and audit findings. Opportunities for continual improvement must also be reviewed.

**Management Review Results**:
The outcomes of the management review must include decisions and actions related to opportunities for continual improvement and any necessary changes to the AI management system. These results must be documented and retained as evidence of the review process.

### **Improvement**

**Continual Improvement**:
The organisation is required to continually improve the suitability, adequacy, and effectiveness of its AI management system. This means that the system should not remain static but should evolve in response to internal evaluations, external changes, and opportunities for enhancement. The goal is to ensure that the AI management system remains aligned with the organization’s objectives and continues to support responsible and effective use of AI.

**Nonconformity and Corrective Action**:
When a nonconformity occurs, the organization must take action to address it. This includes reacting to the issue by controlling and correcting it, and dealing with any consequences. The organization must also evaluate whether further action is needed to eliminate the root cause of the nonconformity to prevent recurrence. This involves reviewing the nonconformity, identifying its causes, and determining whether similar issues exist or could arise elsewhere.

Once actions are identified, they must be implemented and their effectiveness reviewed. If necessary, changes should be made to the AI management system. All actions taken, including the nature of the nonconformities and the results of any corrective actions, must be documented and retained as evidence.

### **Annex A (normative) - Reference Control Objectives and Controls**

##### **A.1 General**
This annex provides a reference set of control objectives and controls to support the implementation and operation of an AI management system. Organizations are not required to implement all listed controls but should select those relevant to their context, risks, and objectives. Additional or alternative controls may also be used.

##### **A.2 Policies Related to AI**
**Objective**: To provide management direction and support for AI systems according to business requirements.
- **A.2.2 AI policy**: The organization shall document a policy for the development or use of AI systems.
- **A.2.3 Alignment with other organizational policies**: The organization shall determine where other policies intersect with AI objectives.
- **A.2.4 Review of the AI policy**: The AI policy shall be reviewed periodically to ensure its continued relevance and effectiveness.

##### **A.3 Internal Organization**
**Objective**: To establish accountability within the organization for responsible AI system management.
- **A.3.2 AI roles and responsibilities**: Roles and responsibilities for AI shall be clearly defined and allocated.
- **A.3.3 Reporting of concerns**: A process shall be in place for reporting concerns related to AI systems.

##### **A.4 Resources for AI Systems**
**Objective**: To ensure all AI system resources are identified and managed to address risks and impacts.
- **A.4.2 Resource documentation**
- **A.4.3 Data resources**
- **A.4.4 Tooling resources**
- **A.4.5 System and computing resources**
- **A.4.6 Human resources**

##### **A.5 Assessing Impacts of AI Systems**
**Objective**: To assess impacts on individuals, groups, and society throughout the AI system life cycle.
- **A.5.2 AI system impact assessment process**
- **A.5.3 Documentation of AI system impact assessments**
- **A.5.4 Assessing AI system impact on individuals or groups**
- **A.5.5 Assessing societal impacts of AI systems**

##### **A.6 AI System Life Cycle**
**Objective**: To define criteria and requirements for each stage of the AI system life cycle.
- **A.6.1.2 Objectives for responsible development**
- **A.6.1.3 Processes for responsible design and development**
- **A.6.2.2 AI system requirements and specification**
- **A.6.2.3 Documentation of design and development**
- **A.6.2.4 Verification and validation**
- **A.6.2.5 Deployment**
- **A.6.2.6 Operation and monitoring**
- **A.6.2.7 Technical documentation**
- **A.6.2.8 Recording of event logs**

##### **A.7 Data for AI Systems**
**Objective**: To manage data throughout the AI system life cycle.
- **A.7.2 Data for development and enhancement**
- **A.7.3 Acquisition of data**
- **A.7.4 Quality of data**
- **A.7.5 Data provenance**
- **A.7.6 Data preparation**

##### A.8 Information for Interested Parties**
**Objective**: To ensure stakeholders have the necessary information to assess AI system risks and impacts.
- **A.8.2 System documentation and user information**
- **A.8.3 External reporting**
- **A.8.4 Communication of incidents**
- **A.8.5 Information for interested parties**

##### **A.9 Use of AI Systems**
**Objective**: To ensure responsible use of AI systems in line with organizational policies.
- **A.9.2 Processes for responsible use**
- **A.9.3 Objectives for responsible use**
- **A.9.4 Intended use of the AI system**

##### **A.10 Third-Party and Customer Relationships**
**Objective**: To manage responsibilities and risks when third parties are involved in the AI system life cycle.
- **A.10.2 Allocating responsibilities**
- **A.10.3 Suppliers**
- **A.10.4 Customers**

### **Annex B (Normative) – Implementation Guidance for AI Controls**

##### **B.1 General**
Annex B provides implementation guidance for the controls listed in Annex A. This guidance is not mandatory and does not need to be included in the statement of applicability. Organizations may adapt or extend the guidance to suit their specific needs and risk treatment strategies. The annex serves as a starting point for developing organization-specific implementations of controls.

**B.2 Policies Related to AI**
**B.2.1 Objective**
To provide management direction and support for AI systems according to business requirements.

**B.2.2 AI Policy**
The AI policy should be informed by business strategy, organizational values, risk appetite, legal requirements, and stakeholder impacts. It should include guiding principles and processes for handling exceptions and should reference other relevant policies.

**B.2.3 Alignment with Other Organizational Policies**
Organizations should identify intersections between AI and other domains such as quality, security, safety, and privacy. Policies should be updated or cross-referenced accordingly.

**B.2.4 Review of the AI Policy**
The AI policy should be reviewed periodically or when necessary. A designated role should be responsible for this review, which should consider changes in the business, legal, or technical environment.

##### **B.3 Internal Organization**

**B.3.1 Objective**
To establish accountability for the implementation, operation, and management of AI systems.

**B.3.2 AI Roles and Responsibilities**
Roles should be defined and assigned based on organizational needs. Areas requiring clear responsibilities include risk management, impact assessments, data quality, oversight, and compliance.

**B.3.3 Reporting of Concerns**
A process should be in place for reporting concerns, with options for anonymity and protection from retaliation. Reports should be escalated appropriately and handled by qualified personnel.

##### **B.4 Resources for AI Systems**

**B.4.1 Objective**
To ensure that all AI system resources are identified and managed to address risks and impacts.

**B.4.2 Resource Documentation**
Organizations should document all relevant resources, including data, tools, systems, and human expertise, using diagrams or architecture models where helpful.

**B.4.3 Data Resources**
Documentation should include data provenance, update history, categories, labeling processes, intended use, quality, retention policies, and known biases.

**B.4.4 Tooling Resources**
Tooling includes algorithms, models, optimization methods, and development tools. Documentation should cover their use and purpose.

**B.4.5 System and Computing Resources**
Details should include hardware specifications, deployment environments, and environmental impacts. Different stages of the AI life cycle may require different resources.

**B.4.6 Human Resources**
Organizations should document the expertise required for each stage of the AI life cycle, including oversight, development, and maintenance. Diversity and inclusion may be relevant depending on the system’s context.


### **Annex C - Potential AI-related organisational objectives and risk sources**